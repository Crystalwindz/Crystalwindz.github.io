title: 从零开始机器学习
categories:
  - 开拓视野
tags:
  - 算法
  - 学习
date: 2018-05-02 20:20:08
---

## 机器学习的定义
作为机器学习领域的先驱，Arthur Samuel在 IBM Journal of Research and Development期刊上发表了一篇名为《Some Studies in Machine Learning Using the Game of Checkers》的论文中，将机器学习非正式定义为：“在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。”

而我认为机器学习是：我们在日常生活中，会遇到很多问题，大致分为两类，一类问题有确定的解法，比如说判断一个数是否是偶数，这种问题可以很清楚地用计算机编写程序并解决；另一类问题则没有固定的解法，比如说判断一个人说了什么，写了什么，因为这些问题的随机性很大，我们解决这些问题的方法，就是用大量的数据，采用一些训练方法，来训练计算机，使得计算机能够解决这些问题，而这个训练过程就称为机器学习。
<!--more-->

## 机器学习的范围
* **模式识别**：模式识别≈机器学习。机器学习的目的就是为了模式识别。
* **数据挖掘**：数据挖掘=机器学习+数据库。数据挖掘是从大量的数据中挖掘出有用的价值，而机器学习的方法则可以用于进行数据挖掘。
* **计算机视觉**：计算机视觉=机器学习+图像处理。手写输入、图片识别在日常生活已经很常见，应用很广泛。
* **语音识别**：语音识别=机器学习+语音处理。听歌识曲、语音识别都是相关应用。
* **自然语言处理**：自然语言处理=机器学习+文本处理。自然语言处理方面，大量使用了编译原理相关的技术，而在自然语言理解方面，则使用了机器学习等技术。
* 等等……

## 机器学习的分类
机器学习大致可以分为**监督学习**、**无监督学习**、**强化学习**、**迁移学习**：
* **监督学习**：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程。通俗来讲，就是让计算机做选择题，并**提供了它们的正确答案**，计算机努力调整自己的参数，最终让自己推测的答案与标准答案一致。
* **无监督学习**：根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，对训练样本进行建模，并进行归类。通俗来讲，就是让计算机做选择题，但**并不提供正确答案**，让计算机自己去分析这些题目之间的关系，并对这些题目进行分类，以后遇到新的问题就计算这个新问题的相似度，根据相似度来对新问题归类。
* **强化学习**：所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号(强化信号)函数值最大，强化学习不同于连接主义学习中的监督学习，主要表现在教师信号上，强化学习中由环境提供的强化信号是对产生动作的好坏作一种评价(通常为标量信号)，而不是告诉强化学习系统RLS如何去产生正确的动作。通俗来讲，就是让计算机做选择题，但**并不提供正确答案**，在计算机做完这些题后，我们**作为老师来批改计算机做的对不对**，对的越多，奖励越多，计算机则努力调整自己的参数来得到更多的奖励，可以理解为先无监督后有监督学习。
* **迁移学习**：迁移学习顾名思义就是就是把已训练好的模型参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率而不用像大多数网络那样从零学习。

## 机器学习的方法
1. **回归算法**：回归算法是通过一系列已知的自变量和因变量，建立变量之间的回归方程，并把回归方程作为算法模型来预测新自变量与因变量的关系。回归分为很多类型，这里介绍两种重要的类型：

    * 线性回归：线性回归用最佳的拟合直线来在因变量和一个或多个自变量之间建立一种关系，想要获得这条拟合直线，可以采用最小二乘法或梯度下降法。线性回归是人们常用的一种方法，它有一些注意点：

        * 自变量与因变量之间必须有线性关系。
        * 多元回归存在多重共线性，自相关性和异方差性。
        * 线性回归对异常值非常敏感，异常值会严重影响回归线，最终影响预测值。
        * 多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定。
        * 在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

    * 逻辑回归：逻辑回归和线性回归很类似，但它和线性回归解决的问题类型不同。线性回归处理的是数值问题，最后得到的是数值，而逻辑回归属于分类算法，最后得到的是离散的分类。由于逻辑回归是分类算法，因此它广泛的用于分类问题，并且不要求自变量和因变量是线性关系，不过，它需要大量的样本，并且自变量不应该相互关联，不然会影响它的估计效果。

    除了这两种，回归模型还有很多，我们在选择合适的回归模型时，应考虑一些因素：
    
    * 数据是模型组成的必然部分，识别变量的关系和影响是很重要的。
    * 比较各个模型的特点，分析它们之间的指标参数，检查你的模型中可能出现的偏差。
    * 交叉验证是评估模型的最好方法，将你的数据分为两份，一份做训练，一份做验证，计算观测值和预测值的一个简单均方差来衡量你的预测精度。
    * 选择模型也取决于你的目的，有时候，一个并不那么强大但容易实现的模型可能对你来说更好。

2. **神经网络**：神经网络算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，借着“深度学习”的热度，神经网络重新成为最强大的机器学习算法之一。

    正如其名，神经网络的诞生源于对大脑工作原理的研究。科学家渴望利用机器模拟大脑，让机器也能思考，经过研究，他们发现人能思考的原因在于人体的神经网络，外部刺激通过神经末梢，转化为电信号，转导到神经元，无数神经元构成神经中枢，神经中枢综合各种信号，做出判断，进而人对外界刺激做出反应。

    在这个过程中，思考的基础是神经元，那么只要我们能够“人造”神经元，就可以模拟神经网络的思考了，而这个“人造神经元”模型，我们称为“感知器”。感知器简单来说，就是接受外部输入，并产生输出，外部输入决定着输出。外部输入是怎么决定输出的呢？可能会有多个外部输入，那么我们赋予每一个输入一个权重，根据权重和输入数据计算出一个结果，将这个结果和一个我们指定的阈值相比，他们之间的大小关系就决定了最后的输出。

    一个神经网络的搭建，需要满足三个条件：
    
    * 输入和输出
    * 权重和阈值
    * 多层感知器的结构
    
    外界给神经网络输入，底层的感知器对外部输入做出判断后，将输出作为上层感知器的输入，直到得到最后的结果。在这个过程中，最困难的就是确定权重和阈值，这两个值通常很难估计，都是主观给出的，我们必须有一种方法来确定它们的值，这种办法就是试错法。

    我们不改变其他的参数，每次只对权重和阈值进行微调，并观察输出的变化，直至得到对应最精确输出的那组权重和阈值，这个过程称为模型的训练。于是神经网络的运作过程如下：

    1. 确定输入和输出
    2. 找到一种或多种算法，可以从输入得到输出
    3. 找到一组已知答案的数据集，用来训练模型，估算权重和阈值
    4. 一旦新的数据产生，输入模型，就可以得到结果，同时对权重和阈值进行校正

    可以看到，整个过程需要大量的计算，这也是直到近几年神经网络才火起来的原因，在原来的年代，硬件是没有这么强的计算力的。

    神经网络在日常生活中有很多例子，语音识别，手写识别，车牌识别，图片识别都是通过输入语音，笔迹，车牌，图片，对它们的各个参数设置权重，再找到一种图片对比算法，声音对比算法作为感知器，最后得到一个概率来决定这个输入的最终结果是什么。我们可以用一组已经人为识别好的数据，来作为训练数据，不断调整参数，直到得到想要的模型，并且日后的新输入还可以对已有的模型进行校正。

3. **SVM(支持向量机)**：支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。从某种意义上来说，SVM是逻辑回归算法的强化，可以获得比逻辑回归更好的分类界线，这其中的关键就在于一个核函数。

    核函数，它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在这个新的高维空间中可以被线性划分为两类，即在空间内线性划分，划分后再将分割的超平面映射回去，就能得到非常复杂的分类界线，从而达成很好的的分类效果。

4. **聚类算法**：聚类就是按照某个特定的标准把一个数据集分割为不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。

    而聚类算法就是对那些无法明确数据分类的数据进行聚类，最后把相似的数据聚在一起，聚类算法不需要用训练数据进行学习，只需要一个可以计算相似度的算法就可以工作了，所以它属于无监督算法。

    聚类的过程大概如下：

    1. 数据准备：包括特征标准化和降维
    2. 特征选择：从最初的特征中选择最有效的特征,并将其存储于向量中
    3. 特征提取：通过对所选择的特征进行转换形成新的突出特征
    4. 聚类：首先选择合适特征类型的某种距离函数进行接近程度的度量，而后执行聚类
    5. 结果评估：主要有3种，外部有效性评估、内部有效性评估和相关性测试评估

    聚类的方法有以下几种：

    * 基于层次的聚类：透过一种层次架构方式，反复将数据进行分裂或聚合，算法流程如下:

        1. 将每个对象看作一类，计算两两之间的最小距离
        2. 将距离最小的两个类合并成一个新类
        3. 重新计算新类与所有类之间的距离
        4. 重复2、3，直到所有类最后合并成一类

        这个算法的可解释性高，不过时间复杂度过高。

    * 基于划分的聚类：预先指定聚类数目或聚类中心，反复迭代逐步降低目标函数误差值直至收敛，得到最终结果，经典的K-means算法流程如下：

        1. 随机地选择k个对象，每个对象初始地代表了一个簇的中心
        2. 对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇
        3. 重新计算每个簇的平均值，更新为新的簇中心
        4. 不断重复2、3，直到准则函数收敛

        这个算法对于大型数据集简单高效，时间复杂度、空间复杂度低，但需要预先设定k值，对最先的k个点选取很敏感，且数据集大时结果容易局部最优。

    * 基于密度的聚类：只要邻近区域的密度（对象或数据点的数目）超过某个阈值，就继续聚类，DBSCAN算法的流程如下：

        1. 从任一对象点p开始
        2. 寻找并合并核心p对象直接密度可达（eps）的对象
        3. 如果p是一个核心点，则找到了一个聚类，如果p是一个边界点（即从p没有密度可达的点）则寻找下一个对象点
        4. 重复2、3，直到所有点都被处理

        这个算法对噪声不敏感；能发现任意形状的聚类，但聚类的结果与参数有很大的关系。

    * 基于网络的聚类：将数据空间划分为网格单元，将数据对象集映射到网格单元中，并计算每个单元的密度。根据预设的阈值判断每个网格单元是否为高密度单元，由邻近的稠密单元组形成”类“。算法的核心流程如下：

        1. 划分网格 
        2. 使用网格单元内数据的统计信息对数据进行压缩表达 
        3. 基于这些统计信息判断高密度网格单元 
        4. 最后将相连的高密度网格单元识别为簇

        这个算法速度很快，但参数敏感，无法处理不规则分布的数据。

    * 基于模型的聚类：为每簇假定了一个模型，寻找数据对给定模型的最佳拟合，同一“类”的数据属于同一种概率分布，即假设数据是根据潜在的概率分布生成的。算法流程如下：

        1. 网络初始化，对输出层每个节点权重赋初值
        2. 将输入样本中随机选取输入向量，找到与输入向量距离最小的权重向量
        3. 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢
        4. 提供新样本，进行训练
        5. 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果

        这个算法对”类“的划分不那么”坚硬“，而是以概率形式表现，但执行效率不高。

    * 还有一些其他的聚类方法，这里不做介绍。

5. **降维算法**：降维算法就是把数据从高维降低到低维层次，这里的维数就是数据特征量的数目，比如说一个盒子的属性有长、宽、高、表面积和体积，这是五维的数据，而我们知道，表面积和体积可以通过长宽高算出来，那么我们可以认为这里产生了数据冗余，我们可以把表面积和体积算出来，从而将五维压缩至三维。另外，我们这里进行的是无损压缩，因为数据是冗余的，而在实际的降维中，可能会导致数据的损失，但这些数据并不是白白损失了，通过降维，我们可以压缩数据与提升机器学习其他算法的效率。降维算法主要有四种：

    * Principal Component Analysis
    * Linear Discriminant Analysis
    * Locally :inear Embedding
    * Laplacian Eigenmaps

6. **推荐算法**：推荐算法在我们的日常生活中比较常用，我们手机每天收到的各种推送、推荐消息全都采用了推荐算法。推荐算法的主要用途就是向用户推荐用户可能喜欢的东西，主要有以下几种：

    * 基于用户信息的推荐：根据用户的基本信息，推荐他们可能喜欢的东西。
    * 基于物品信息的推荐：根据用户看过的物品，推荐与该物品相似的东西。
    * 协同过滤推荐：找到和用户具有相似爱好的人，并断定这一群具有相似爱好的人喜欢的东西也相似，基于此推荐用户其他人看过但自己没看过的东西。
    * 混合推荐：顾名思义，就是将以上几种算法混合起来。

7. **其他算法**：机器学习界算法很多，还有很多其他的算法，比如高斯判别、朴素贝叶斯、决策树等算法，这里不做介绍。

介绍了这么多算法，我们来给它们分一下类：

* 监督学习算法： 回归算法，神经网络，SVM
* 无监督学习算法：聚类算法、降维算法
* 特殊算法：推荐算法

## 机器学习的应用

1. **数据挖掘**：数据挖掘就是在大量数据中通过算法找到其中隐藏的信息，而说到数据挖掘，通常与机器学习离不开，因为数据挖掘的很多常用方法都来源于机器学习。
2. **深度学习**：深度学习源于对人工神经网络的研究，具有多隐层的神经网络称为深度神经网络，对深度神经网络的研究称为深度学习，可以说，深度学习是机器学习的一个子类。
3. **人工智能**：人工智能是让计算机模拟人的思维过程和智能行为，人工智能可以说是机器学习的父类，对人工智能的研究过程中机器学习才发展起来。

**参考**

* [浅谈机器学习基础](https://www.jianshu.com/p/ed9ae5385b89)
* [从机器学习谈起](https://www.cnblogs.com/subconscious/p/4107357.html#four)
* [什么是迁移学习](https://www.zhihu.com/question/41979241/answer/123545914)
* [神经网络入门](http://www.ruanyifeng.com/blog/2017/07/neural-network.html)
* [聚类算法](https://blog.csdn.net/abc200941410128/article/details/78541273?locationNum=1&fps=1)
